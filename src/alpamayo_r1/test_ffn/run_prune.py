import torch
import sys
from pathlib import Path

# ------------------------
# ç¡®ä¿èƒ½ import æœ¬åœ° repo
# ------------------------
sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))

from alpamayo_r1.models.alpamayo_r1 import AlpamayoR1
from prune_ffn_alpamayo import prune_alpamayo_ffn

# ------------------------
# é…ç½®
# ------------------------
HF_MODEL_ID = "nvidia/Alpamayo-R1-10B"  # HuggingFace æƒé‡
KEEP_RATIO = 0.7                        # FFN å‰ªææ¯”ä¾‹
OUTPUT_PATH = "alpamayo_r1_ffn70_pruned.pth"  # å‰ªæåæƒé‡ä¿å­˜è·¯å¾„
DEVICE = "cuda"

# ------------------------
# 1ï¸âƒ£ ä¸‹è½½ HF æƒé‡å¹¶åˆå§‹åŒ–æ¨¡å‹
# ------------------------
print(f"â³ Loading Alpamayo-R1 model from HF: {HF_MODEL_ID} ...")
model = AlpamayoR1.from_pretrained(HF_MODEL_ID, dtype=torch.bfloat16).to(DEVICE)
model.eval()
print(f"âœ… Model loaded on {DEVICE}")

# ------------------------
# 2ï¸âƒ£ FFN å‰ªæ
# ------------------------
print(f"â³ Pruning FFN with keep_ratio={KEEP_RATIO} ...")
model = prune_alpamayo_ffn(model, keep_ratio=KEEP_RATIO, verbose=True)
print("âœ… FFN pruning completed")

# ------------------------
# 3ï¸âƒ£ ä¿å­˜å‰ªæåçš„æƒé‡
# ------------------------
torch.save(model.state_dict(), OUTPUT_PATH)
print(f"ğŸ¯ Pruned model saved to {OUTPUT_PATH}")

# ------------------------
# 4ï¸âƒ£ æµ‹è¯•åŠ è½½å‰ªææƒé‡ï¼ˆå¯é€‰ï¼‰
# ------------------------
# åŠ è½½éªŒè¯
# model2 = AlpamayoR1.from_pretrained(HF_MODEL_ID, dtype=torch.bfloat16).to(DEVICE)
# state = torch.load(OUTPUT_PATH, map_location=DEVICE)
# model2.load_state_dict(state, strict=True)
# print("âœ… Pruned weights load test successful")
